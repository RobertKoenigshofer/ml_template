{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=\"mnist_template\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "from src.dataloader.mnist_datamodule import MNISTDataModule\n",
    "\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"mnist_template\"\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(pl.LightningModule):\n",
    "    def __init__(self, lr=0.1, mean=0., sd=1.):\n",
    "        super().__init__()\n",
    "\n",
    "        # initial args\n",
    "        self.lr = lr\n",
    "        self.metric = MulticlassAccuracy(num_classes=10)\n",
    "        self.mean = mean\n",
    "        self.sd = sd\n",
    "\n",
    "        # define model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 784), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(784, 784), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(784, 10)\n",
    "        )\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.flatten(1))\n",
    "\n",
    "    def _common_step(self, batch, batch_idx, stage :str):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            acc = self.metric(logits, y)\n",
    "\n",
    "            self.log(f'{stage}/loss', loss)\n",
    "            self.log(f'{stage}/accuracy', acc)\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self._common_step(batch, batch_idx, 'train')\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, _ = self._common_step(batch, batch_idx, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, logits = self._common_step(batch, batch_idx, 'test')\n",
    "        x, y = batch\n",
    "\n",
    "        images = []\n",
    "        captions = []\n",
    "        for i in range(1):\n",
    "            img = x[i]*self.sd + self.mean\n",
    "            pred = F.softmax(logits[i], dim=0)\n",
    "            images.append(img)\n",
    "            captions.append('predictions: ' + str(pred.detach().cpu().numpy().round(2)))\n",
    "\n",
    "        self.logger.log_image(key='test_predictions', images=images, caption=captions)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkoero\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../logs\\wandb\\run-20231018_163249-jp36xu2j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/koero/mnist_template/runs/jp36xu2j' target=\"_blank\">feedforward</a></strong> to <a href='https://wandb.ai/koero/mnist_template' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/koero/mnist_template' target=\"_blank\">https://wandb.ai/koero/mnist_template</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/koero/mnist_template/runs/jp36xu2j' target=\"_blank\">https://wandb.ai/koero/mnist_template/runs/jp36xu2j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "logger = WandbLogger(\n",
    "    project=\"mnist_template\",\n",
    "    name='feedforward',\n",
    "    log_model=True,\n",
    "    save_dir='../logs',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNISTDataModule()\n",
    "mnist.prepare_data()\n",
    "mnist.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTModel(lr=0.01, mean=mnist.default_mean, sd=mnist.default_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type               | Params\n",
      "----------------------------------------------\n",
      "0 | metric | MulticlassAccuracy | 0     \n",
      "1 | model  | Sequential         | 1.2 M \n",
      "----------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.955     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497b43b9b92741e4afd0785500aa2acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3195d2001e4a4cb5c5211bb127edd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233b3d8e326140f483be02c4591064c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516a746bd15f441fb4c8b836958aaa77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5388896442466f8d960d0eba7a6cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator='gpu',\n",
    "    accumulate_grad_batches=1,\n",
    "    log_every_n_steps=10,\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(\n",
    "    model = model,\n",
    "    datamodule=mnist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f03e5d9df024057b9f2cde69d4317d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9511427879333496     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.17657946050167084    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9511427879333496    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.17657946050167084   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/loss': 0.17657946050167084, 'test/accuracy': 0.9511427879333496}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(\n",
    "    model = model,\n",
    "    datamodule=mnist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5bc0dd4e934c9fa7bedbee9f514ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='14.222 MB of 14.222 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>train/accuracy</td><td>▄▂▁▂▅▃▄▅▄▆█▅▅▆▇▆▆▇▄▆▄▅▇▇▇▇█▆▆▇▅▇▄▆▆▆▆▆▇▇</td></tr><tr><td>train/loss</td><td>▆█▇█▄▆▄▄▄▂▁▃▅▂▄▃▃▂▅▄▄▃▁▁▁▂▁▁▃▂▃▁▄▂▂▃▃▄▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇█████████████████</td></tr><tr><td>val/accuracy</td><td>▄▁█</td></tr><tr><td>val/loss</td><td>▆█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>test/accuracy</td><td>0.95114</td></tr><tr><td>test/loss</td><td>0.17658</td></tr><tr><td>train/accuracy</td><td>0.95682</td></tr><tr><td>train/loss</td><td>0.13785</td></tr><tr><td>trainer/global_step</td><td>1290</td></tr><tr><td>val/accuracy</td><td>0.94785</td></tr><tr><td>val/loss</td><td>0.18277</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feedforward</strong> at: <a href='https://wandb.ai/koero/mnist_template/runs/jp36xu2j' target=\"_blank\">https://wandb.ai/koero/mnist_template/runs/jp36xu2j</a><br/>Synced 5 W&B file(s), 79 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../logs\\wandb\\run-20231018_163249-jp36xu2j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
